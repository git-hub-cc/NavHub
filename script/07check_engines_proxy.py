import os
import json
import requests
import time
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒºåŸŸ =================
# ç›®æ ‡æ–‡ä»¶ä½äº data ç›®å½•ä¸‹
DATA_DIR = '../data'
TARGET_FILE = '00engines.json'

# è¯·æ±‚è®¾ç½®
TIMEOUT = 3  # 3ç§’è¶…æ—¶ï¼Œä»»ä½•è¶…è¿‡æ­¤æ—¶é—´çš„è¯·æ±‚éƒ½å°†è§†ä¸ºå¤±è´¥
MAX_WORKERS = 20  # å¹¶å‘çº¿ç¨‹æ•°

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
}
# ===========================================

def get_domain(url):
    """ä»URLä¸­æå–åŸŸåï¼ˆnetlocï¼‰"""
    try:
        return urlparse(url).netloc
    except Exception:
        return None

def check_domain_connectivity(domain, sample_url):
    """
    æ£€æµ‹åŸŸåçš„è¿é€šæ€§ã€‚
    ä¼˜å…ˆæµ‹è¯•æ ¹åŸŸåï¼Œå¤±è´¥åˆ™å°è¯•å…·ä½“URLã€‚
    è¿”å›: True (éœ€è¦ä»£ç†/è¿æ¥å¤±è´¥), False (è¿æ¥æ­£å¸¸)
    """
    protocol = sample_url.split('://')[0] if '://' in sample_url else 'https'
    test_url = f"{protocol}://{domain}"

    try:
        requests.get(test_url, headers=HEADERS, timeout=TIMEOUT)
        return False
    except requests.exceptions.RequestException:
        if test_url != sample_url:
            try:
                requests.get(sample_url, headers=HEADERS, timeout=TIMEOUT)
                return False
            except requests.exceptions.RequestException:
                pass
        return True

def save_custom_formatted_json(file_path, data):
    """
    è‡ªå®šä¹‰JSONä¿å­˜å‡½æ•°ï¼Œä»¥åŒ¹é…åŸå§‹æ–‡ä»¶çš„ç‰¹å®šæ ¼å¼ã€‚
    - "categories" åˆ—è¡¨ä¸­çš„æ¯ä¸ªå¯¹è±¡å ä¸€è¡Œã€‚
    - "engines" ä¸­æ¯ä¸ªå¼•æ“åˆ—è¡¨é‡Œçš„ç«™ç‚¹å¯¹è±¡å ä¸€è¡Œã€‚
    """
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write('{\n')

        # 1. å¤„ç† categories
        categories = data.get('categories', [])
        category_lines = []
        for cat in categories:
            # ensure_ascii=False ä¿è¯ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
            line = json.dumps(cat, ensure_ascii=False)
            category_lines.append(f'    {line}')

        f.write('  "categories": [\n')
        f.write(',\n'.join(category_lines))
        f.write('\n  ],\n')

        # 2. å¤„ç† engines
        engines = data.get('engines', {})
        engine_blocks = []
        for name, sites in engines.items():
            site_lines = [f'      {json.dumps(site, ensure_ascii=False)}' for site in sites]

            block = f'    "{name}": [\n'
            block += ',\n'.join(site_lines)
            block += '\n    ]'
            engine_blocks.append(block)

        f.write('  "engines": {\n')
        f.write(',\n'.join(engine_blocks))
        f.write('\n  }\n')

        f.write('}\n')


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æµ‹ 00engines.json ä»£ç†çŠ¶æ€...")
    file_path = os.path.join(DATA_DIR, TARGET_FILE)

    if not os.path.exists(file_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ -> {file_path}")
        return

    # 1. åŠ è½½æ•°æ®å¹¶æŒ‰åŸŸåèšåˆ
    domain_map = {}
    domain_sample_url = {}

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        for sites in data.get('engines', {}).values():
            for site in sites:
                url = site.get('url')
                if not url: continue

                domain = get_domain(url)
                if not domain: continue

                if domain not in domain_map:
                    domain_map[domain] = []
                    domain_sample_url[domain] = url

                domain_map[domain].append(site)

    except Exception as e:
        print(f"âŒ è¯»å–æˆ–è§£æJSONæ–‡ä»¶å¤±è´¥: {file_path} - {e}")
        return

    total_domains = len(domain_map)
    if total_domains == 0:
        print("ğŸ¤· æ–‡ä»¶ä¸­æœªå‘ç°æœ‰æ•ˆçš„URLã€‚")
        return

    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæ¯•ï¼Œå…±å‘ç° {total_domains} ä¸ªå”¯ä¸€åŸŸåéœ€è¦æ£€æµ‹ã€‚")
    print(f"â±ï¸  è¶…æ—¶é˜ˆå€¼: {TIMEOUT}ç§’, å¹¶å‘æ•°: {MAX_WORKERS}")

    # 2. å¹¶å‘æ£€æµ‹æ‰€æœ‰åŸŸå
    need_proxy_domains = set()
    checked_count = 0

    print("\nğŸ” å¼€å§‹å¹¶å‘æ£€æµ‹...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_domain = {
            executor.submit(check_domain_connectivity, domain, domain_sample_url[domain]): domain
            for domain in domain_map.keys()
        }

        for future in as_completed(future_to_domain):
            domain = future_to_domain[future]
            checked_count += 1
            progress = (checked_count / total_domains) * 100
            print(f"   è¿›åº¦: {checked_count}/{total_domains} ({progress:.1f}%)", end='\r')

            try:
                if future.result():
                    need_proxy_domains.add(domain)
            except Exception as e:
                print(f"\n   âš ï¸ æ£€æµ‹å¼‚å¸¸ for {domain}: {e}")
                need_proxy_domains.add(domain)

    duration = time.time() - start_time
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œè€—æ—¶ {duration:.2f} ç§’ã€‚")
    print(f"ğŸ”´ å‘ç° {len(need_proxy_domains)} ä¸ªåŸŸåæ— æ³•ç›´è¿ï¼Œå°†è¢«æ ‡è®°ä¸º 'proxy: true'ã€‚")

    # 3. æ›´æ–°JSONå¯¹è±¡ä¸­çš„ 'proxy' å­—æ®µ
    update_count = 0
    for domain, sites_list in domain_map.items():
        needs_proxy = domain in need_proxy_domains
        for site in sites_list:
            if site.get('proxy') is not needs_proxy:
                site['proxy'] = needs_proxy
                update_count += 1

    # 4. å¦‚æœæœ‰å˜åŠ¨ï¼Œåˆ™å†™å›æ–‡ä»¶
    if update_count > 0:
        print(f"ğŸ’¾ å…±æœ‰ {update_count} ä¸ªç«™ç‚¹çš„ 'proxy' çŠ¶æ€å‘ç”Ÿå˜æ›´ï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        try:
            # ä½¿ç”¨è‡ªå®šä¹‰çš„æ ¼å¼åŒ–ä¿å­˜å‡½æ•°
            save_custom_formatted_json(file_path, data)
            print(f"ğŸ‰ æ–‡ä»¶ {file_path} ä¿å­˜æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
    else:
        print("âœ¨ æ‰€æœ‰ç«™ç‚¹çš„ä»£ç†çŠ¶æ€å‡æœªå‘ç”Ÿå˜åŒ–ï¼Œæ— éœ€æ›´æ–°æ–‡ä»¶ã€‚")

if __name__ == "__main__":
    main()