import os
import json
import re
import base64
import requests
import mimetypes
import time
import threading
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒºåŸŸ =================
# æ•°æ®ç›®å½•
DATA_DIR = '../data'
# è¦å¤„ç†çš„æ–‡ä»¶æ‰©å±•å
TARGET_EXT = '.json'
# éœ€è¦æ’é™¤çš„é…ç½®æ–‡ä»¶
EXCLUDED_FILES = [
    'package-lock.json',
    'engines.json',
    'pinyin-map.json'
]

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
}

# è¶…æ—¶è®¾ç½® (ç§’)
TIMEOUT = 10
# Base64 è½¬æ¢çš„å¤§å°é™åˆ¶ (å­—èŠ‚)ï¼Œ50KB
MAX_ICON_SIZE = 50 * 1024
# å¹¶å‘çº¿ç¨‹æ•°
MAX_WORKERS = 20
# åŒä¸€åŸŸåè¯·æ±‚é—´éš” (ç§’)
DOMAIN_REQUEST_INTERVAL = 1.0
# ===========================================

class DomainRateLimiter:
    """
    åŸŸåé€Ÿç‡é™åˆ¶å™¨
    ç”¨äºåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹æ§åˆ¶å¯¹åŒä¸€åŸŸåçš„è¯·æ±‚é¢‘ç‡
    """
    def __init__(self, interval):
        self.interval = interval
        self.domains = {}  # è®°å½•åŸŸåæœ€åè®¿é—®æ—¶é—´ { 'baidu.com': 1680000000.00 }
        self.lock = threading.Lock() # çº¿ç¨‹é”ï¼Œä¿è¯å­—å…¸è¯»å†™å®‰å…¨

    def wait_if_needed(self, url):
        """
        è§£æURLä¸­çš„åŸŸåï¼Œå¦‚æœè¯¥åŸŸåæœ€è¿‘åˆšè¢«è®¿é—®è¿‡ï¼Œåˆ™å¼ºåˆ¶ä¼‘çœ 
        """
        try:
            domain = urlparse(url).netloc
            if not domain:
                return

            wait_time = 0

            with self.lock:
                last_time = self.domains.get(domain, 0)
                now = time.time()
                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´ï¼š é—´éš” - (å½“å‰æ—¶é—´ - ä¸Šæ¬¡æ—¶é—´)
                diff = now - last_time
                if diff < self.interval:
                    wait_time = self.interval - diff
                    # æ›´æ–°è¯¥åŸŸåçš„é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆå½“å‰æ—¶é—´ + éœ€è¦ç­‰å¾…çš„æ—¶é—´ï¼‰
                    # è¿™æ ·ä¸‹ä¸€ä¸ªçº¿ç¨‹è¿›æ¥æ—¶ï¼Œä¼šåŸºäºè¿™ä¸ªæ›´æ–°åçš„æ—¶é—´è®¡ç®—ç­‰å¾…
                    self.domains[domain] = now + wait_time
                else:
                    self.domains[domain] = now

            # åœ¨é”å¤–éƒ¨è¿›è¡Œä¼‘çœ ï¼Œé¿å…é˜»å¡å…¶ä»–é’ˆå¯¹ä¸åŒåŸŸåçš„çº¿ç¨‹
            if wait_time > 0:
                time.sleep(wait_time)

        except Exception:
            # è§£æå‡ºé”™åˆ™ä¸é™åˆ¶ï¼Œé¿å…é˜»æ–­æµç¨‹
            pass

# åˆå§‹åŒ–å…¨å±€é™æµå™¨
rate_limiter = DomainRateLimiter(DOMAIN_REQUEST_INTERVAL)

def get_target_files(directory):
    """è·å–ç›®å½•ä¸‹æ‰€æœ‰éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶"""
    files = []
    if not os.path.exists(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return []

    for filename in os.listdir(directory):
        if filename.endswith(TARGET_EXT) and filename not in EXCLUDED_FILES:
            files.append(os.path.join(directory, filename))
    return files

def image_to_base64(content, content_type):
    """å°†äºŒè¿›åˆ¶å›¾ç‰‡å†…å®¹è½¬æ¢ä¸ºBase64å­—ç¬¦ä¸²"""
    try:
        base64_data = base64.b64encode(content).decode('utf-8')
        return f"data:{content_type};base64,{base64_data}"
    except Exception as e:
        print(f"   âš ï¸ Base64è½¬æ¢å¤±è´¥: {e}")
        return None

def find_favicon_url(html_content, base_url):
    """ä»HTMLå†…å®¹ä¸­æ­£åˆ™æå–faviconé“¾æ¥"""
    icon_patterns = [
        r'<link[^>]*?rel=["\'](?:shortcut )?icon["\'][^>]*?href=["\'](.*?)["\']',
        r'<link[^>]*?href=["\'](.*?)["\'][^>]*?rel=["\'](?:shortcut )?icon["\']',
        r'<link[^>]*?rel=["\']apple-touch-icon["\'][^>]*?href=["\'](.*?)["\']'
    ]

    for pattern in icon_patterns:
        match = re.search(pattern, html_content, re.IGNORECASE)
        if match:
            href = match.group(1).strip()
            if href.startswith('//'):
                return 'https:' + href
            elif href.startswith('data:image'):
                return None
            else:
                return urljoin(base_url, href)
    return None

def process_site_node(site):
    """
    å¤„ç†å•ä¸ªç«™ç‚¹èŠ‚ç‚¹çš„æ ¸å¿ƒé€»è¾‘
    """
    original_url = site.get('url', '')
    title = site.get('title', 'æœªçŸ¥æ ‡é¢˜')

    if not original_url:
        return None

    # 1. å¼ºåˆ¶å‡çº§ HTTPS
    target_url = original_url
    if target_url.startswith('http://'):
        target_url = target_url.replace('http://', 'https://', 1)

    try:
        # === é™æµä»‹å…¥ ===
        # åœ¨å‘èµ·ä»»ä½•è¯·æ±‚å‰ï¼Œå…ˆæ£€æŸ¥è¯¥åŸŸåæ˜¯å¦éœ€è¦ç­‰å¾…
        rate_limiter.wait_if_needed(target_url)
        # =============

        # 2. éªŒè¯ç«™ç‚¹è¿é€šæ€§
        response = requests.get(target_url, headers=HEADERS, timeout=TIMEOUT)

        if response.status_code >= 400:
            print(f"âŒ [åˆ é™¤] çŠ¶æ€ç å¼‚å¸¸ {response.status_code}: {title}")
            return None

        site['url'] = response.url

        # 3. æå– Favicon
        icon_url = find_favicon_url(response.text, response.url)

        if not icon_url:
            parsed_uri = urlparse(response.url)
            base_domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)
            icon_url = urljoin(base_domain, 'favicon.ico')

        # 4. ä¸‹è½½å¹¶è½¬æ¢å›¾æ ‡
        if icon_url:
            # ä¸‹è½½å›¾æ ‡å‰ï¼Œä¹Ÿéœ€è¦é’ˆå¯¹å›¾æ ‡æ‰€åœ¨çš„åŸŸåè¿›è¡Œé™æµæ£€æŸ¥
            # å› ä¸ºå›¾æ ‡å¾€å¾€å’Œä¸»ç«™åœ¨åŒä¸€ä¸ªåŸŸåä¸‹
            rate_limiter.wait_if_needed(icon_url)

            try:
                icon_resp = requests.get(icon_url, headers=HEADERS, timeout=5)
                if icon_resp.status_code == 200:
                    content_type = icon_resp.headers.get('Content-Type', '')
                    content_size = len(icon_resp.content)

                    if 'image' not in content_type:
                        ext = os.path.splitext(icon_url)[1]
                        content_type = mimetypes.types_map.get(ext.lower(), 'image/x-icon')

                    # å¤§å°æ£€æŸ¥
                    if content_size > MAX_ICON_SIZE:
                        print(f"   âš ï¸ [ä¿ç•™URL] å›¾æ ‡è¿‡å¤§ ({content_size/1024:.1f}KB): {title}")
                        site['icon'] = icon_url
                    else:
                        base64_icon = image_to_base64(icon_resp.content, content_type)
                        if base64_icon:
                            site['icon'] = base64_icon
                            print(f"   âœ… [Base64] å›¾æ ‡æ›´æ–°æˆåŠŸ: {title}")
            except Exception as e:
                # å›¾æ ‡ä¸‹è½½å¤±è´¥ä¸åº”å¯¼è‡´èŠ‚ç‚¹åˆ é™¤ï¼Œä¿ç•™åŸçŠ¶æˆ–ä¸å¤„ç†
                print(f"   âš ï¸ å›¾æ ‡ä¸‹è½½å¤±è´¥: {title} - {e}")

    except requests.exceptions.SSLError:
        print(f"âŒ [åˆ é™¤] SSL/HTTPS è¯ä¹¦é”™è¯¯: {title}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ [åˆ é™¤] æ— æ³•è®¿é—®: {title} - {type(e).__name__}")
        return None
    except Exception as e:
        print(f"âŒ [åˆ é™¤] æœªçŸ¥é”™è¯¯: {title} - {e}")
        return None

    return site

def process_file(file_path):
    """è¯»å–å¹¶å¤„ç†å•ä¸ªJSONæ–‡ä»¶"""
    print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–JSONå¤±è´¥: {file_path} - {e}")
        return

    if 'categories' not in data:
        print(f"âš ï¸ è·³è¿‡: æ–‡ä»¶æ ¼å¼ä¸ç¬¦åˆè§„èŒƒ (ç¼ºå°‘ categories å­—æ®µ)")
        return

    total_sites_before = 0
    valid_sites_count = 0
    new_categories = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for category in data['categories']:
            cat_name = category.get('categoryName', 'æœªçŸ¥åˆ†ç±»')
            sites = category.get('sites', [])

            if not sites:
                continue

            total_sites_before += len(sites)
            print(f"  ğŸ‘‰ æ­£åœ¨å¤„ç†åˆ†ç±»: {cat_name} ({len(sites)} ä¸ªç«™ç‚¹)...")

            # æäº¤ä»»åŠ¡
            futures = [executor.submit(process_site_node, site) for site in sites]

            valid_sites = []
            for future in as_completed(futures):
                result = future.result()
                if result:
                    valid_sites.append(result)

            if valid_sites:
                category['sites'] = valid_sites
                new_categories.append(category)
                valid_sites_count += len(valid_sites)
            else:
                print(f"  ğŸ—‘ï¸ åˆ†ç±» [{cat_name}] å·²æ¸…ç©ºï¼Œå°†è¢«ç§»é™¤ã€‚")

    data['categories'] = new_categories

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜æˆåŠŸ: {file_path}")
        print(f"ğŸ“Š ç»Ÿè®¡: åŸæœ‰ {total_sites_before} -> ç°æœ‰ {valid_sites_count} (åˆ é™¤äº† {total_sites_before - valid_sites_count} ä¸ªæ— æ•ˆèŠ‚ç‚¹)")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {file_path} - {e}")

def main():
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†å¯¼èˆªæ•°æ®...")
    print(f"ğŸ“Œ è§„åˆ™: ä»…ä¿ç•™æ”¯æŒHTTPSçš„ç«™ç‚¹ï¼Œå†…è”Base64å›¾æ ‡")
    print(f"â³ é™æµ: åŒä¸€åŸŸåé—´éš” {DOMAIN_REQUEST_INTERVAL} ç§’")

    files = get_target_files(DATA_DIR)

    if not files:
        print("ğŸ¤· æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„JSONæ–‡ä»¶ã€‚")
        return

    for file_path in files:
        process_file(file_path)

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()