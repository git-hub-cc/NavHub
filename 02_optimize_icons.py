import os
import json
import re
import base64
import hashlib
import io
import requests
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from PIL import Image

# ================= é…ç½®åŒºåŸŸ =================
DATA_DIR = 'data'
IMG_DIR = 'img'
TARGET_HEIGHT = 42
IMAGE_QUALITY = 80
MAX_WORKERS = 16
TARGET_EXT = '.json'
EXCLUDED_FILES = ['package-lock.json', 'engines.json', 'pinyin-map.json']

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
}
# ===========================================

def ensure_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")

def get_target_files(directory):
    files = []
    if not os.path.exists(directory):
        return []
    for filename in os.listdir(directory):
        if filename.endswith(TARGET_EXT) and filename not in EXCLUDED_FILES:
            files.append(os.path.join(directory, filename))
    return files

def get_image_bytes(source_str, site_title):
    """
    æ ¹æ®è¾“å…¥çš„å­—ç¬¦ä¸²è·å–å›¾ç‰‡çš„äºŒè¿›åˆ¶æ•°æ®
    å¢åŠ  site_title å‚æ•°ç”¨äºæ—¥å¿—å®šä½
    """
    if not source_str:
        return None

    # 1. å¤„ç† Base64
    if source_str.startswith('data:image'):
        try:
            if ',' in source_str:
                header, encoded = source_str.split(',', 1)

                # --- å¢å¼ºå¥å£®æ€§ï¼šæ¸…ç†éæ³•å­—ç¬¦ ---
                # æŸäº› Base64 å¯èƒ½åŒ…å«æ¢è¡Œç¬¦ \n æˆ–ç©ºæ ¼ï¼Œéœ€æ¸…ç†
                clean_encoded = re.sub(r'[^a-zA-Z0-9+/=]', '', encoded)

                return base64.b64decode(clean_encoded)
        except Exception as e:
            print(f"   âŒ [Base64é”™è¯¯] ç½‘ç«™: {site_title}")
            print(f"      åŸå› : {e}")
            # æ‰“å°éƒ¨åˆ†å­—ç¬¦ä¸²ä»¥ä¾¿è°ƒè¯•ï¼ˆå‰50ä¸ªå­—ç¬¦ï¼‰
            print(f"      æ•°æ®ç‰‡æ®µ: {source_str[:50]}...")
            return None

    # 2. å¤„ç† URL
    if source_str.startswith('http'):
        try:
            resp = requests.get(source_str, headers=HEADERS, timeout=10)
            if resp.status_code == 200:
                return resp.content
        except Exception:
            return None

    return None

def process_and_save_image(image_bytes, site_title):
    """å¤„ç†å¹¶ä¿å­˜å›¾ç‰‡"""
    if not image_bytes:
        return None

    try:
        md5_hash = hashlib.md5(image_bytes).hexdigest()
        filename = f"{md5_hash}.webp"
        save_path = os.path.join(IMG_DIR, filename)
        web_path = f"{IMG_DIR}/{filename}"

        if os.path.exists(save_path):
            return web_path

        with Image.open(io.BytesIO(image_bytes)) as img:
            if img.mode in ('CMYK', 'P', '1'):
                img = img.convert('RGBA')
            if img.mode == 'RGB':
                img = img.convert('RGBA')

            aspect_ratio = img.width / img.height
            new_width = int(TARGET_HEIGHT * aspect_ratio)

            if img.height != TARGET_HEIGHT:
                img = img.resize((new_width, TARGET_HEIGHT), Image.Resampling.LANCZOS)

            img.save(save_path, 'WEBP', quality=IMAGE_QUALITY)

        return web_path

    except Exception as e:
        print(f"   âš ï¸ [å›¾ç‰‡å¤„ç†å¤±è´¥] ç½‘ç«™: {site_title}")
        print(f"      åŸå› : {e}")
        return None

def process_site_node(site):
    """å¤„ç†å•ä¸ªç«™ç‚¹èŠ‚ç‚¹"""
    original_icon = site.get('icon', '')
    site_title = site.get('title', 'æœªçŸ¥æ ‡é¢˜') # è·å–æ ‡é¢˜ç”¨äºæ—¥å¿—

    if not original_icon:
        return False

    if original_icon.startswith(f"{IMG_DIR}/"):
        if os.path.exists(original_icon):
            return False
        return False

    # ä¼ é€’ site_title
    img_bytes = get_image_bytes(original_icon, site_title)

    if img_bytes:
        # ä¼ é€’ site_title
        new_path = process_and_save_image(img_bytes, site_title)
        if new_path:
            site['icon'] = new_path
            return True

    return False

def process_file(file_path):
    print(f"\nğŸ“‚ å¤„ç†æ–‡ä»¶: {file_path}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return

    if 'categories' not in data:
        return

    changed_count = 0
    tasks = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for category in data['categories']:
            sites = category.get('sites', [])
            for site in sites:
                tasks.append(executor.submit(process_site_node, site))

        for future in as_completed(tasks):
            if future.result():
                changed_count += 1

    if changed_count > 0:
        print(f"   ğŸ’¾ æ›´æ–°äº† {changed_count} ä¸ªå›¾æ ‡ï¼Œæ­£åœ¨ä¿å­˜...")
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
    else:
        print("   âœ¨ æ— éœ€æ›´æ–°")

def main():
    print("ğŸš€ å¼€å§‹å›¾ç‰‡æœ¬åœ°åŒ–ä¸å‹ç¼©å¤„ç†...")
    ensure_dir(IMG_DIR)
    files = get_target_files(DATA_DIR)

    if not files:
        print("ğŸ¤· æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return

    for file_path in files:
        process_file(file_path)

    print("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()