import os
import json
import requests
import time
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒºåŸŸ =================
DATA_DIR = 'data'
TARGET_EXT = '.json'
EXCLUDED_FILES = ['package-lock.json', 'engines.json', 'pinyin-map.json']

# è¯·æ±‚è®¾ç½®
TIMEOUT = 3  # 3ç§’è¶…æ—¶ï¼Œä»»ä½•è¶…è¿‡æ­¤æ—¶é—´çš„è¯·æ±‚éƒ½å°†è§†ä¸ºå¤±è´¥
MAX_WORKERS = 20 # å¹¶å‘çº¿ç¨‹æ•°ï¼Œç½‘ç»œæ£€æµ‹æ˜¯IOå¯†é›†å‹ï¼Œå¯ä»¥è®¾é«˜ä¸€ç‚¹

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
}
# ===========================================

def get_target_files(directory):
    """è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶è·¯å¾„"""
    files = []
    if not os.path.exists(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return []
    for filename in os.listdir(directory):
        if filename.endswith(TARGET_EXT) and filename not in EXCLUDED_FILES:
            files.append(os.path.join(directory, filename))
    return files

def get_domain(url):
    """ä»URLä¸­æå–åŸŸåï¼ˆnetlocï¼‰"""
    try:
        parsed = urlparse(url)
        return parsed.netloc
    except Exception:
        return None

def check_domain_connectivity(domain, sample_url):
    """
    æ£€æµ‹åŸŸåçš„è¿é€šæ€§
    è¿”å›: True (éœ€è¦ä»£ç†/è¿æ¥å¤±è´¥), False (è¿æ¥æ­£å¸¸)
    """
    # ç­–ç•¥ï¼šä¼˜å…ˆæ„å»ºæ ¹åŸŸåè¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºæ ¹åŸŸåé€šå¸¸æœ€å¿«
    # å¦‚æœ sample_url æœ¬èº«å°±æ˜¯æ ¹åŸŸåï¼Œåˆ™ç›´æ¥ç”¨
    try:
        protocol = sample_url.split('://')[0]
        test_url = f"{protocol}://{domain}"
    except:
        test_url = sample_url

    try:
        # ä½¿ç”¨ HEAD è¯·æ±‚å¯ä»¥å‡å°‘æµé‡ï¼Œä½†æœ‰äº›æœåŠ¡å™¨ä¸æ”¯æŒ HEADï¼ŒGET æ›´ç¨³å¦¥
        # timeout è®¾ç½®ä¸º 3 ç§’
        requests.get(test_url, headers=HEADERS, timeout=TIMEOUT)
        return False # æ­£å¸¸è¿æ¥ï¼Œä¸éœ€è¦ä»£ç†
    except Exception:
        # å†æ¬¡å°è¯•ï¼šå¦‚æœæ ¹åŸŸåå¤±è´¥ï¼Œå°è¯•åŸæœ¬å…·ä½“çš„ sample_url
        if test_url != sample_url:
            try:
                requests.get(sample_url, headers=HEADERS, timeout=TIMEOUT)
                return False
            except Exception:
                pass
        return True # å¤±è´¥ï¼Œè§†ä¸ºéœ€è¦ä»£ç†

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡ä»£ç†æ£€æµ‹...")
    print(f"â±ï¸  è¶…æ—¶é˜ˆå€¼: {TIMEOUT}ç§’")

    files = get_target_files(DATA_DIR)
    if not files:
        print("ğŸ¤· æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return

    # 1. åŠ è½½æ‰€æœ‰æ•°æ®å¹¶æŒ‰åŸŸåå»ºç«‹æ˜ å°„
    # ç»“æ„: { "www.google.com": [ {site_node1}, {site_node2} ], ... }
    domain_map = {}
    # è®°å½•æ¯ä¸ªåŸŸåçš„ä¸€ä¸ªä»£è¡¨æ€§ URLï¼Œç”¨äºæµ‹è¯•
    domain_sample_url = {}
    # è®°å½•æ–‡ä»¶å¯¹è±¡ï¼Œä»¥ä¾¿æœ€åä¿å­˜ { "filepath": json_data_object }
    file_data_map = {}

    print("ğŸ“Š æ­£åœ¨åŠ è½½å¹¶èšåˆæ•°æ®...")
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                file_data_map[file_path] = data

                if 'categories' not in data:
                    continue

                for category in data['categories']:
                    sites = category.get('sites', [])
                    for site in sites:
                        url = site.get('url', '')
                        if not url: continue

                        domain = get_domain(url)
                        if not domain: continue

                        if domain not in domain_map:
                            domain_map[domain] = []
                            domain_sample_url[domain] = url

                        domain_map[domain].append(site)

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {e}")

    total_domains = len(domain_map)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæ¯•ï¼Œå…±å‘ç° {total_domains} ä¸ªå”¯ä¸€åŸŸåéœ€è¦æ£€æµ‹ã€‚")

    # 2. å¹¶å‘æ£€æµ‹åŸŸå
    need_proxy_domains = set()
    checked_count = 0

    print("\nğŸ” å¼€å§‹å¹¶å‘æ£€æµ‹...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰åŸŸåçš„æ£€æµ‹ä»»åŠ¡
        future_to_domain = {
            executor.submit(check_domain_connectivity, domain, domain_sample_url[domain]): domain
            for domain in domain_map.keys()
        }

        for future in as_completed(future_to_domain):
            domain = future_to_domain[future]
            checked_count += 1

            # è¿›åº¦æ¡æ•ˆæœ
            if checked_count % 50 == 0:
                print(f"   è¿›åº¦: {checked_count}/{total_domains}...", end='\r')

            try:
                is_need_proxy = future.result()
                if is_need_proxy:
                    need_proxy_domains.add(domain)
                    # æ‰“å°è¶…æ—¶è®°å½•
                    # print(f"   ğŸŒ [è¶…æ—¶/å¤±è´¥] {domain}")
            except Exception:
                need_proxy_domains.add(domain)

    duration = time.time() - start_time
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œè€—æ—¶ {duration:.2f}ç§’ã€‚")
    print(f"ğŸ”´ å‘ç° {len(need_proxy_domains)} ä¸ªåŸŸåæ— æ³•ç›´è¿ï¼Œéœ€å¼€å¯ä»£ç†ã€‚")

    # 3. æ‰¹é‡æ›´æ–°æ•°æ®
    update_count = 0
    for domain in need_proxy_domains:
        site_list = domain_map[domain]
        for site in site_list:
            # åªæœ‰å½“ proxy åŸæœ¬ä¸º false æˆ–ä¸å­˜åœ¨æ—¶æ‰æ›´æ–°ï¼Œé¿å…é‡å¤æ“ä½œ
            if not site.get('proxy', False):
                site['proxy'] = True
                update_count += 1

    if update_count > 0:
        print(f"ğŸ’¾ æ›´æ–°äº† {update_count} ä¸ªç«™ç‚¹èŠ‚ç‚¹çš„ proxy å­—æ®µï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶...")

        # 4. å†™å›æ–‡ä»¶
        for file_path, data in file_data_map.items():
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                # print(f"   å·²ä¿å­˜: {file_path}")
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {file_path} - {e}")
        print("ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼")
    else:
        print("âœ¨ æ²¡æœ‰å‘ç°æ–°çš„éœ€è¦ä»£ç†çš„ç«™ç‚¹ã€‚")

if __name__ == "__main__":
    main()