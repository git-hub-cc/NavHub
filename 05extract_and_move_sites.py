import os
import json
from urllib.parse import urlparse
import tldextract # éœ€è¦å…ˆå®‰è£… pip install tldextract

# ================= é…ç½®åŒºåŸŸ =================
# å’Œå…¶ä»–è„šæœ¬ä¿æŒä¸€è‡´
DATA_DIR = 'data'
TARGET_EXT = '.json'
EXCLUDED_FILES = ['package-lock.json', 'engines.json', 'pinyin-map.json']

# è¦è¿ç§»çš„åŸŸååˆ—è¡¨ (ä½¿ç”¨é›†åˆ set ä»¥è·å¾—æ›´å¿«çš„æŸ¥æ‰¾é€Ÿåº¦)
TARGET_DOMAINS_TO_MIGRATE = {
    'www.aliyundrive.com',
    'wwi.lanzoui.com',
    'www.yuque.com',
    'baozangku.lanzoui.com',
    'pan.baidu.com',
    'mp.weixin.qq.com',
    'www.lanzoui.com',
    'baozangku.lanzoux.com',
    'flowus.cn',
    'github.com',
    'www.lanzoul.com',
    'url67.ctfile.com',
    'chendandan.lanzoux.com'
}

# è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILENAME = '00èµ„æº.json'
# ===========================================

def get_target_files(directory):
    """è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶è·¯å¾„"""
    files = []
    if not os.path.exists(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return []
    for filename in os.listdir(directory):
        if filename.endswith(TARGET_EXT) and filename not in EXCLUDED_FILES:
            files.append(os.path.join(directory, filename))
    return files

def get_domain(url):
    """ä»URLä¸­æå–åŸŸåï¼ˆnetlocï¼‰"""
    try:
        if '://' not in url:
            url = 'http://' + url
        parsed = urlparse(url)
        return parsed.netloc
    except Exception:
        return None

def get_root_domain(domain):
    """
    ä½¿ç”¨ tldextract è·å–æ ¹åŸŸå
    ä¾‹å¦‚: a.b.c.com -> c.com
          a.b.com.cn -> b.com.cn
    """
    if not domain:
        return ""
    extracted = tldextract.extract(domain)
    # top_domain_under_public_suffix æ˜¯ 'registered_domain' çš„æ–°åç§°ï¼ŒåŠŸèƒ½ç›¸åŒ
    return extracted.top_domain_under_public_suffix

def main():
    print("ğŸš€ å¼€å§‹è¿ç§»æŒ‡å®šåŸŸåçš„ç½‘ç«™èŠ‚ç‚¹...")
    print(f"ğŸ¯ å°†è¿ç§» {len(TARGET_DOMAINS_TO_MIGRATE)} ä¸ªæŒ‡å®šåŸŸååŠå…¶å­åŸŸåçš„èŠ‚ç‚¹ã€‚")

    files = get_target_files(DATA_DIR)
    if not files:
        print("ğŸ¤· æœªæ‰¾åˆ°ä»»ä½• JSON æ•°æ®æ–‡ä»¶ã€‚")
        return

    migrated_nodes = []
    modified_files_count = 0

    print("\nğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶å¹¶æå–èŠ‚ç‚¹...")
    # --- é˜¶æ®µä¸€: éå†æ‰€æœ‰æ–‡ä»¶ï¼Œæå–ç›®æ ‡èŠ‚ç‚¹å¹¶ä»æºæ–‡ä»¶åˆ é™¤ ---
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            file_was_modified = False

            if 'categories' not in data:
                continue

            for category in data.get('categories', []):
                sites_to_keep = []
                original_sites = category.get('sites', [])

                for site in original_sites:
                    url = site.get('url')
                    if not url:
                        sites_to_keep.append(site)
                        continue

                    domain = get_domain(url)
                    if domain in TARGET_DOMAINS_TO_MIGRATE:
                        # è¿™æ˜¯ä¸€ä¸ªç›®æ ‡èŠ‚ç‚¹ï¼Œæ·»åŠ åˆ°è¿ç§»åˆ—è¡¨
                        migrated_nodes.append(site)
                        file_was_modified = True
                    else:
                        # éç›®æ ‡èŠ‚ç‚¹ï¼Œä¿ç•™åœ¨åŸå¤„
                        sites_to_keep.append(site)

                # ç”¨è¿‡æ»¤åçš„åˆ—è¡¨æ›¿æ¢åŸæ¥çš„ sites åˆ—è¡¨
                category['sites'] = sites_to_keep

            # å¦‚æœæ–‡ä»¶è¢«ä¿®æ”¹è¿‡ï¼Œåˆ™å†™å›æ–‡ä»¶
            if file_was_modified:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                modified_files_count += 1
                print(f"   âœï¸ å·²ä¿®æ”¹: {os.path.basename(file_path)}")

        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {file_path} - {e}")

    print(f"\nâœ… èŠ‚ç‚¹æå–å’Œæºæ–‡ä»¶æ›´æ–°å®Œæˆã€‚å…±æå– {len(migrated_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œä¿®æ”¹äº† {modified_files_count} ä¸ªæ–‡ä»¶ã€‚")

    if not migrated_nodes:
        print("âœ¨ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦è¿ç§»çš„èŠ‚ç‚¹ã€‚")
        return

    # --- é˜¶æ®µäºŒ: å¯¹æå–çš„èŠ‚ç‚¹è¿›è¡Œæ’åº ---
    print("\nğŸ”„ æ­£åœ¨å¯¹æå–çš„èŠ‚ç‚¹è¿›è¡Œæ’åº...")
    # æ’åºè§„åˆ™:
    # 1. æŒ‰æ ¹åŸŸå (e.g., lanzoui.com) æ’åº
    # 2. åœ¨æ ¹åŸŸåç›¸åŒçš„æƒ…å†µä¸‹ï¼ŒæŒ‰å®Œæ•´å­åŸŸå (e.g., baozangku.lanzoui.com) æ’åº
    sorted_nodes = sorted(
        migrated_nodes,
        key=lambda site: (
            get_root_domain(get_domain(site.get('url', ''))),
            get_domain(site.get('url', ''))
        )
    )
    print("âœ… æ’åºå®Œæˆã€‚")

    # --- é˜¶æ®µä¸‰: å°†æ’åºåçš„èŠ‚ç‚¹å†™å…¥æ–°æ–‡ä»¶ ---
    output_path = os.path.join(DATA_DIR, OUTPUT_FILENAME)
    print(f"\nğŸ’¾ æ­£åœ¨å°† {len(sorted_nodes)} ä¸ªèŠ‚ç‚¹å†™å…¥æ–°æ–‡ä»¶: {output_path}")

    # æ„å»ºæ–°æ–‡ä»¶çš„ JSON ç»“æ„
    new_data = {
        "name": "è¿ç§»çš„åˆé›†",
        "description": f"è¯¥æ–‡ä»¶åŒ…å«ä»å…¶ä»–æ–‡ä»¶ä¸­è‡ªåŠ¨è¿ç§»è¿‡æ¥çš„ {len(sorted_nodes)} ä¸ªç½‘ç«™èŠ‚ç‚¹ã€‚",
        "categories": [
            {
                "name": "å…¨éƒ¨è¿ç§»å†…å®¹",
                "description": "æŒ‰ä¸»åŸŸåå’Œå­åŸŸåæ’åºã€‚",
                "sites": sorted_nodes
            }
        ]
    }

    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(new_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ‰ æˆåŠŸåˆ›å»ºè¿ç§»æ–‡ä»¶: {output_path}")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–°æ–‡ä»¶å¤±è´¥: {e}")

    print("\nâœ¨ å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼")


if __name__ == "__main__":
    main()