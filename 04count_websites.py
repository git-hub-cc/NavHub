import os
import json
from urllib.parse import urlparse
from collections import Counter

# ================= é…ç½®åŒºåŸŸ =================
# å’Œ 03_check_proxy.py è„šæœ¬ä¿æŒä¸€è‡´ï¼Œæ–¹ä¾¿ç”¨æˆ·ç†è§£
DATA_DIR = 'data'
TARGET_EXT = '.json'
EXCLUDED_FILES = ['package-lock.json', 'engines.json', 'pinyin-map.json']
# ===========================================

def get_target_files(directory):
    """è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶è·¯å¾„ (å¤ç”¨è‡ª 03_check_proxy.py)"""
    files = []
    if not os.path.exists(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return []
    for filename in os.listdir(directory):
        if filename.endswith(TARGET_EXT) and filename not in EXCLUDED_FILES:
            files.append(os.path.join(directory, filename))
    return files

def get_domain(url):
    """ä»URLä¸­æå–åŸŸåï¼ˆnetlocï¼‰(å¤ç”¨è‡ª 03_check_proxy.py å¹¶ä¼˜åŒ–)"""
    try:
        # urlparse éœ€è¦ä¸€ä¸ªåè®®å¤´ (scheme) æ‰èƒ½æ­£ç¡®è§£æ netloc
        # å¦‚æœ URL ä¸­æ²¡æœ‰ï¼Œåˆ™ä¸ºå…¶æ·»åŠ ä¸€ä¸ªé»˜è®¤çš„
        if '://' not in url:
            url = 'http://' + url
        parsed = urlparse(url)
        return parsed.netloc
    except Exception:
        return None

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºç»Ÿè®¡å„åŸŸåå‡ºç°çš„æ¬¡æ•°"""
    print("ğŸš€ å¼€å§‹ç»Ÿè®¡ data ç›®å½•ä¸‹çš„åŸŸåæ•°é‡...")

    files = get_target_files(DATA_DIR)
    if not files:
        print("ğŸ¤· æœªæ‰¾åˆ°ä»»ä½• JSON æ•°æ®æ–‡ä»¶ã€‚")
        return

    # ä½¿ç”¨ collections.Counter æ¥é«˜æ•ˆåœ°è¿›è¡Œè®¡æ•°
    domain_counts = Counter()
    total_sites_processed = 0

    print(f"ğŸ” æ­£åœ¨å¤„ç† {len(files)} ä¸ªæ–‡ä»¶...")

    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ ¹æ® 03_check_proxy.py çš„ç»“æ„ï¼Œæ•°æ®åœ¨ 'categories' -> 'sites' -> 'url'
            if 'categories' not in data:
                continue

            for category in data.get('categories', []):
                for site in category.get('sites', []):
                    url = site.get('url')
                    if not url:
                        continue

                    total_sites_processed += 1
                    domain = get_domain(url)

                    if domain:
                        # Counter ä¼šè‡ªåŠ¨å¤„ç†æ–°åŸŸåå¹¶å¢åŠ å·²æœ‰åŸŸåçš„è®¡æ•°
                        domain_counts[domain] += 1
                    else:
                        print(f"âš ï¸ æ— æ³•ä» '{url}' è§£æåŸŸå (æ–‡ä»¶: {file_path})")

        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æå¤±è´¥: {file_path} - {e}")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {file_path} - {e}")

    print("\nâœ… æ•°æ®å¤„ç†å®Œæ¯•ï¼")
    print(f"ğŸ“Š å…±æ‰«æ {total_sites_processed} ä¸ªç«™ç‚¹æ¡ç›®ï¼Œå‘ç° {len(domain_counts)} ä¸ªç‹¬ç«‹åŸŸåã€‚")

    if not domain_counts:
        print("ğŸ¤· æœªèƒ½ä»æ–‡ä»¶ä¸­ç»Ÿè®¡å‡ºä»»ä½•åŸŸåã€‚")
        return

    print("\n--- åŸŸåæ•°é‡ç»Ÿè®¡ (æŒ‰å‡ºç°æ¬¡æ•°é™åº) ---")
    # domain_counts.most_common() è¿”å›ä¸€ä¸ªæŒ‰è®¡æ•°å€¼é™åºæ’åºçš„ (å…ƒç´ , è®¡æ•°å€¼) åˆ—è¡¨
    for domain, count in domain_counts.most_common():
        # ä½¿ç”¨æ ¼å¼åŒ–å­—ç¬¦ä¸²å¯¹é½è¾“å‡ºï¼Œä½¿ç»“æœæ›´ç¾è§‚
        print(f"{domain:<30} | {count} æ¬¡")

if __name__ == "__main__":
    main()